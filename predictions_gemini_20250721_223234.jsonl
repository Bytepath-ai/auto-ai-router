{"instance_id": "astropy__astropy-12907", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/astropy/modeling/core.py\n+++ b/astropy/modeling/core.py\n@@ -1113,20 +1113,21 @@\n     def __init__(self, op, left, right, name=None, n_inputs_slotted=None):\n         self._op = op\n         self._left = left\n         self._right = right\n-\n         self._name = name\n         self._n_inputs = n_inputs_slotted\n \n-        if op in ['&', '|', '@']:\n+        # The separability matrix of a compound model is determined\n+        # recursively, and the recursion terminates when a model is\n+        # not a CompoundModel. So we need to flatten nested compound\n+        # models with & or | operators.\n+        if op in ['&', '|']:\n+            # The _tree property will re-balance the tree\n+            left, right = self._tree\n+            self._left = left\n+            self._right = right\n+        elif op == '@':\n             if op == '@':\n                 op = '|'\n-            tree = self._tree\n-            if tree:\n-                self._op, self._left, self._right = tree\n \n         self._initialize_slices()\n \n@@ -1678,22 +1679,19 @@\n         # TODO: It might be better to somehow make this a joint_inverse\n         # property of the two models, but that is a bigger change.\n         if self._op == '&':\n-            if isinstance(self._left, CompoundModel) and self._left.op == '&':\n-                left = self._left.left\n-                right = self._left.right & self._right\n-                # To be determined what to do with inverses\n-                return ('&', left, right)\n-            elif isinstance(self._right, CompoundModel) and self._right.op == '&':\n-                right_left = self._right.left\n-                right_right = self._right.right\n-                left = self._left & right_left\n-                right = right_right\n-                # To be determined what to do with inverses\n-                return ('&', left, right)\n+            tree = self\n+            while isinstance(tree._left, CompoundModel) and tree._left.op == '&':\n+                tree = CompoundModel('&', tree._left.left, tree._left.right & tree._right)\n+            return tree._left, tree._right\n         elif self._op == '|':\n-            if isinstance(self._left, CompoundModel) and self._left.op == '|':\n-                left = self._left.left\n-                right = self._left.right | self._right\n-                return ('|', left, right)\n-\n-        return None\n+            tree = self\n+            while isinstance(tree._left, CompoundModel) and tree._left.op == '|':\n+                tree = CompoundModel('|', tree._left.left, tree._left.right | tree._right)\n+            return tree._left, tree._right\n+        # The other operators do not need to be flattened as they can't be\n+        # nested, so we just return the left and right models.\n+        return self._left, self._right\n\n```"}
{"instance_id": "astropy__astropy-14182", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@ -23,10 +23,10 @@\n     _format_name = \"rst\"\n     _description = \"reStructuredText simple table\"\n \n-    def __init__(self):\n+    def __init__(self, header_rows=None):\n         super().__init__()\n-        self.header = RSTHeader()\n+        self.header = RSTHeader(header_rows=header_rows)\n         self.data = RSTData()\n \n     def write(self, table, output):\n@@ -49,33 +49,42 @@\n     def write(self, table, output):\n         \"\"\"\n         Write the header for a reST simple table.  This consists of:\n+\n         - line of '=' spanning all columns\n-        - column names\n+        - header rows (e.g. column names, units)\n         - line of '=' spanning all columns\n+\n         \"\"\"\n         self.data.set_col_starts_ends(table)\n-        widths = [\n-            self.data.get_col_width(col) for col in self.data.base_reader.cols\n-        ]\n+        widths = [self.data.get_col_width(col) for col in self.data.base_reader.cols]\n         if self.data.formats:\n             for col in self.data.base_reader.cols:\n                 if col.name in self.data.formats:\n                     col.format = self.data.formats[col.name]\n \n         # Write header\n         self.write_header_line(widths, output)\n-        self.write_col_names(widths, output)\n+\n+        if self.header_rows:\n+            for row_spec in self.header_rows:\n+                if row_spec == \"name\":\n+                    vals = [col.name for col in self.data.base_reader.cols]\n+                elif row_spec == \"unit\":\n+                    vals = [str(col.unit or \"\") for col in self.data.base_reader.cols]\n+                elif row_spec == \"dtype\":\n+                    vals = [str(col.dtype) for col in self.data.base_reader.cols]\n+                elif row_spec == \"format\":\n+                    vals = [str(col.format or \"\") for col in self.data.base_reader.cols]\n+                elif row_spec == \"description\":\n+                    vals = [str(col.description or \"\") for col in self.data.base_reader.cols]\n+                else:\n+                    raise ValueError(f\"row_spec {row_spec} not supported for writing\")\n+\n+                val_strs = [\n+                    self.data.stringify_val(val, col)\n+                    for val, col in zip(vals, self.data.base_reader.cols)\n+                ]\n+                val_fmts = [val.center(width) for val, width in zip(val_strs, widths)]\n+                output.write(\" \".join(val_fmts) + \"\\n\")\n+\n         self.write_header_line(widths, output)\n \n     def write_header_line(self, widths, output):\n         line = \" \".join([\"=\" * width for width in widths])\n         output.write(line + \"\\n\")\n-\n-    def write_col_names(self, widths, output):\n-        col_names = [\n-            col.name.center(width) for col, width in zip(self.data.base_reader.cols, widths)\n-        ]\n-        output.write(\" \".join(col_names) + \"\\n\")\n \n \n class RSTData(fixed_width.FixedWidthData):\n\n```"}
{"instance_id": "astropy__astropy-14365", "model_name_or_path": "gemini-2.5-pro", "model_patch": "", "error": "The model response did not complete successfully.\nFinish reason: 2.\nFinish message: .\nSafety ratings: [].\nTo protect the integrity of the chat session, the request and response were not added to chat history.\nTo skip the response validation, specify `model.start_chat(response_validation=False)`.\nNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service."}
{"instance_id": "astropy__astropy-14995", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/astropy/nddata/mixins/ndarithmetic.py\n+++ b/astropy/nddata/mixins/ndarithmetic.py\n@@ -515,4 +515,6 @@\n         if self.mask is None:\n             return deepcopy(operand.mask)\n \n-        return handle_mask(self.mask, operand.mask)\n+        elif operand.mask is None:\n+            return deepcopy(self.mask)\n+        else:\n+            return handle_mask(self.mask, operand.mask)\n\n```"}
{"instance_id": "astropy__astropy-6938", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -358,7 +358,7 @@\n \n             # Replace exponent separator in floating point numbers\n             if 'D' in format:\n-                output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+                output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n \n             # Now, justify the field\n             output_field = np.char.array(output_field).ljust(col.width)\n\n```"}
{"instance_id": "astropy__astropy-7746", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1220,6 +1220,12 @@\n                 if not all(x.shape == axes[0].shape for x in axes):\n                     raise ValueError(\"Input arrays must be of the same shape\")\n \n+                # Handle empty arrays\n+                if self.naxis > 0 and axes[0].size == 0:\n+                    return [np.array([], dtype=np.float_)\n+                            for i in range(self.naxis)]\n+\n                 xy = np.vstack(axes)\n                 if ra_dec_order and sky == 'input':\n                     xy = self._denormalize_sky(xy)\n@@ -1245,6 +1251,11 @@\n                     \"When providing a single array, it must be 2-dimensional\")\n \n             def _return_array(xy, origin):\n+                # Handle empty arrays\n+                if xy.shape[0] == 0:\n+                    return np.empty((0, self.naxis), dtype=np.float_)\n+\n                 if ra_dec_order and sky == 'input':\n                     xy = self._denormalize_sky(xy)\n                 output = func(xy, origin)\n\n```"}
{"instance_id": "django__django-10914", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -463,7 +463,7 @@\n     'django.core.files.uploadhandler.TemporaryFileUploadHandler',\n ]\n FILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n-FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644\n FILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n \n # Media files\n--- a/docs/howto/deployment/checklist.txt\n+++ b/docs/howto/deployment/checklist.txt\n@@ -165,27 +165,22 @@\n \n ``FILE_UPLOAD_PERMISSIONS``\n ---------------------------\n-\n-.. versionchanged:: 2.1\n-\n-    The behavior of ``FILE_UPLOAD_PERMISSIONS`` was clarified.\n-\n-If your file upload settings allow users to upload files, you are **strongly**\n-encouraged to set this value to something sensible for your environment. The\n-default of ``None`` may result in inconsistent permissions for uploaded files,\n-depending on the upload handler and the ``umask`` of your process.\n-\n-On most platforms, files uploaded with\n-:class:`~django.core.files.uploadhandler.TemporaryFileUploadHandler` will be\n-saved with permissions of ``0o600``. Files uploaded with\n-:class:`~django.core.files.uploadhandler.MemoryFileUploadHandler` will have\n-permissions determined by your process's ``umask``.\n-\n-By default, Django uses a combination of these handlers. Small files are\n-processed in memory, and large files are written to disk. This means that the\n-size of a user's upload will determine the permissions of the saved file.\n-\n-Setting this value to ``0o644`` (or ``0o640``) is often desirable. This ensures\n-that uploaded files are readable by the user running the Django application, and\n-optionally by other members of the user's group.\n+ \n+.. versionchanged:: 3.0\n+ \n+    The default value of ``FILE_UPLOAD_PERMISSIONS`` was changed to ``0o644``.\n+ \n+By default, Django saves uploaded files with permissions of ``0o644``. This\n+ensures that uploaded files are readable by the user running the Django\n+application, and readable by other users. If you need different permissions,\n+you can change this setting. For example, ``0o640`` would make files readable\n+only by the user running the Django application and members of its group.\n+ \n+If you set this to ``None``, you will get operating-system dependent behavior\n+which may result in inconsistent permissions for uploaded files, depending on\n+the upload handler and the ``umask`` of your process. On most platforms, files\n+uploaded with\n+:class:`~django.core.files.uploadhandler.TemporaryFileUploadHandler` will be\n+saved with permissions of ``0o600``. Files uploaded with\n+:class:`~django.core.files.uploadhandler.MemoryFileUploadHandler` will have\n+permissions determined by your process's ``umask``.\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -2620,14 +2620,15 @@\n \n FILE_UPLOAD_PERMISSIONS\n =======================\n-\n-Default: ``None``\n-\n The numeric mode (e.g. ``0o644``) to set newly uploaded files to.\n \n-If this isn't given or is ``None``, you'll get operating-system dependent\n-behavior. On most platforms, temporary files will have a mode of ``0o600``, and\n-files saved from memory will be saved using the system's standard umask.\n+Default: ``0o644``\n+\n+If you set this to ``None``, you'll get operating-system dependent behavior. On\n+most platforms, files saved from a temporary file will have a mode of\n+``0o600``, and files saved from memory will be saved using the system's\n+standard umask. Using the default value is recommended to ensure consistent\n+permissions for all uploaded files.\n \n For more details, see :ref:`file-upload-permissions`.\n \n@@ -2638,9 +2639,8 @@\n The numeric mode (e.g. ``0o755``) to set newly created directories to when\n uploading files.\n \n-This value mirrors the functionality and caveats of the\n-:setting:`FILE_UPLOAD_PERMISSIONS` setting.\n+If this isn't given or is ``None``, you'll get operating-system dependent\n+behavior based on the process's umask.\n \n .. setting:: MEDIA_ROOT\n \n--- a/docs/releases/3.0.txt\n+++ b/docs/releases/3.0.txt\n@@ -35,6 +35,13 @@\n \n Backwards incompatible changes in 3.0\n =====================================\n+\n+* The default value of the :setting:`FILE_UPLOAD_PERMISSIONS` setting is now\n+  ``0o644``. Previously it was ``None``, which resulted in file permissions\n+  that depended on the file upload handler being used and the process's\n+  ``umask``. To restore the previous behavior, set\n+  :setting:`FILE_UPLOAD_PERMISSIONS` to ``None``.\n+\n \n ``django.utils.encoding.force_text()`` and ``smart_text()``\n -----------------------------------------------------------\n\n```"}
{"instance_id": "django__django-10924", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -1100,6 +1100,8 @@\n class FilePathField(ChoiceField):\n     def __init__(self, path, match=None, recursive=False, allow_files=True,\n                  allow_folders=False, **kwargs):\n+        if callable(path):\n+            path = path()\n         self.path = path\n         self.match = match\n         self.recursive = recursive\n\n```"}
{"instance_id": "django__django-11001", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -40,7 +40,7 @@\n         # used for compile().\n         self.pre_sql_setup()\n \n-        self.ordering_parts = re.compile(r'(.*)\\s+(?:ASC|DESC)', re.IGNORECASE)\n+        self.ordering_parts = re.compile(r'(.*)\\s+(?:ASC|DESC)', re.IGNORECASE | re.DOTALL)\n \n     def pre_sql_setup(self):\n         \"\"\"\n\n```"}
{"instance_id": "django__django-11019", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -1,5 +1,6 @@\n \"HTML Widget classes\"\n \n+import warnings\n from copy import deepcopy\n \n from django.conf import settings\n@@ -11,6 +12,10 @@\n from django.utils.safestring import mark_safe\n from django.utils.translation import gettext_lazy as _\n \n+\n+class MediaOrderConflictWarning(RuntimeWarning):\n+    pass\n+\n # A list of time formats that are accepted by TimeField.\n TIME_INPUT_FORMATS = [\n     '%H:%M:%S',  # '14:30:59'\n@@ -46,16 +51,23 @@\n             self._css_lists = []\n             self._js_lists = []\n \n-        if css:\n-            self._css_lists.extend(css.items() if isinstance(css, dict) else [(None, css)])\n-        if js:\n-            self._js_lists.append(js)\n+        if css:\n+            css_items = css.items() if isinstance(css, dict) else [(None, css)]\n+            for medium, path_list in css_items:\n+                if not isinstance(path_list, (list, tuple)):\n+                    path_list = [path_list]\n+                self._css_lists.append((medium, list(dict.fromkeys(path_list))))\n+        if js:\n+            if not isinstance(js, (list, tuple)):\n+                js = [js]\n+            self._js_lists.append(list(dict.fromkeys(js)))\n \n     def __repr__(self):\n         return 'Media(css=%r, js=%r)' % (self.css, self.js)\n \n     def __str__(self):\n         return self.render()\n \n     def render(self):\n         return self.render_js() + self.render_css()\n@@ -94,76 +106,161 @@\n     @property\n     def _js(self):\n         if not self._js_lists:\n             return []\n-        # filter(None, ...) avoids calling merge() with empty lists.\n-        js = self._js_lists[0]\n-        for obj in filter(None, self._js_lists[1:]):\n-            js = self.merge(js, obj)\n-        return js\n+        return self.merge_js(self._js_lists)\n \n     @property\n     def _css(self):\n-        css = {}\n-        for medium, list_of_files in self._css_lists:\n-            if medium not in css:\n-                css[medium] = []\n-            for f in list_of_files:\n-                if f not in css[medium]:\n-                    css[medium].append(f)\n-        return css\n+        if not self._css_lists:\n+            return {}\n+        return self.merge_css(self._css_lists)\n \n     def __add__(self, other):\n         new = Media()\n         new._css_lists = self._css_lists + other._css_lists\n         new._js_lists = self._js_lists + other._js_lists\n         return new\n \n     @staticmethod\n-    def merge(list1, list2):\n-        \"\"\"\n-        Merge two lists of scripts. The general idea is to return a list of\n-        scripts that's a combination of both lists, with no duplicates. If a\n-        script is in both lists, it will be returned at the first position it\n-        appears in either list.\n-        \"\"\"\n-        if not list1:\n-            return list2\n-        if not list2:\n-            return list1\n-\n-        # The merge logic is subtle. It is strongly inspired by\n-        # https://github.com/seb-m/py-generic-merge\n-        # We are trying to preserve the order of both lists.\n-        # For example, if we have list1=[a, b, c] and list2=[a, d, c],\n-        # we want to return [a, b, d, c] and not [a, d, b, c].\n-        # We can do that by finding the longest common subsequence (LCS) of\n-        # both lists. The non-LCS items are then inserted at the right place.\n-        # In our example, LCS is [a, c]. The non-LCS items are [b] and [d].\n-        # [b] is between a and c, so we insert it there.\n-        # [d] is between a and c, so we insert it there.\n-        # The only undefined behavior is where to insert items that are at the\n-        # same place. For example, if list1=[a, b, c] and list2=[a, d, c],\n-        # then both b and d are between a and c. We can insert b then d, or\n-        # d then b. The current implementation favors list1, so it will be\n-        # b then d.\n-        def _merge(list1, list2):\n-            if not list1:\n-                return list2, []\n-            if not list2:\n-                return list1, []\n-\n-            merged, conflicts = [], []\n-            sentinel = object()\n-            i1 = i2 = 0\n-            while i1 < len(list1) or i2 < len(list2):\n-                item1 = list1[i1] if i1 < len(list1) else sentinel\n-                item2 = list2[i2] if i2 < len(list2) else sentinel\n-                if item1 == item2:\n-                    merged.append(item1)\n-                    i1 += 1\n-                    i2 += 1\n-                elif item1 in list2[i2:]:\n-                    if item2 in list1[i1:]:\n-                        conflicts.append((item1, item2))\n-                        # Favor list1 by default.\n-                        merged.append(item1)\n-                        i1 += 1\n-                    else:\n-                        merged.append(item2)\n-                        i2 += 1\n-                else:\n-                    merged.append(item1)\n-                    i1 += 1\n-            return merged, conflicts\n-\n-        merged, conflicts = _merge(list1, list2)\n-        if conflicts:\n-            warnings.warn(\n-                'Detected duplicate Media files in an opposite order:\\n' +\n-                '\\n'.join('  %s\\n  %s' % c for c in conflicts),\n-                MediaOrderConflictWarning, stacklevel=2\n-            )\n-        return merged\n+    def merge_js(lists):\n+        \"\"\"\n+        Merge lists of scripts, while preserving order and removing duplicates.\n+        \"\"\"\n+        if not lists:\n+            return []\n+\n+        # A list of all items that have been seen, in the order they appear\n+        # in the resulting list.\n+        seen = []\n+        # A list of all items that have been included in the resulting list.\n+        result = []\n+\n+        # The dependency graph is a dictionary mapping items to a list of\n+        # items that must come after it.\n+        dependencies = {}\n+\n+        # The list of items that have no dependencies, and can be added to\n+        # the result list.\n+        roots = []\n+\n+        # A list of all unique items.\n+        all_items = []\n+\n+        # A dictionary mapping items to the list of input lists that they\n+        # appear in.\n+        lists_containing = {}\n+\n+        # First, gather all unique items and their locations.\n+        for lst in lists:\n+            # The list was already deduplicated in __init__() or __add__().\n+            for item in lst:\n+                if item not in all_items:\n+                    all_items.append(item)\n+                    lists_containing[item] = []\n+                lists_containing[item].append(lst)\n+\n+        # Build the dependency graph.\n+        for item in all_items:\n+            dependencies[item] = []\n+\n+        for lst in lists:\n+            for i, item in enumerate(lst):\n+                # Every item later in a list is a dependency of the current\n+                # item.\n+                for dependency in lst[i + 1:]:\n+                    dependencies[item].append(dependency)\n+\n+        # Find the roots (items that are not dependencies of any other item).\n+        dependency_items = {\n+            dep for deps in dependencies.values() for dep in deps\n+        }\n+        for item in all_items:\n+            if item not in dependency_items:\n+                roots.append(item)\n+\n+        # The main loop.\n+        while roots:\n+            # The order of items in roots is not significant, so sort them to\n+            # ensure a consistent output.\n+            roots.sort()\n+            root = roots.pop(0)\n+\n+            if root in seen:\n+                continue\n+            seen.append(root)\n+\n+            # Look for cycles.\n+            for lst in lists_containing[root]:\n+                try:\n+                    i = lst.index(root)\n+                    for prev_item in lst[:i]:\n+                        if prev_item not in seen:\n+                            # This is a dependency cycle.\n+                            warnings.warn(\n+                                'Detected duplicate Media files in an '\n+                                'opposite order:\\n%s\\n%s' % (root, prev_item),\n+                                MediaOrderConflictWarning,\n+                                stacklevel=2,\n+                            )\n+                            # Break the cycle by removing the dependency.\n+                            dependencies[prev_item].remove(root)\n+                except ValueError:\n+                    # The root is not in this list, so we don't need to\n+                    # check for cycles.\n+                    pass\n+\n+            result.append(root)\n+\n+            # Update the dependency graph.\n+            for dependency in sorted(dependencies.pop(root, [])):\n+                # If the dependency is not a dependency of any other item,\n+                # it is now a root.\n+                is_dependency = any(\n+                    dependency in deps for deps in dependencies.values()\n+                )\n+                if not is_dependency:\n+                    roots.append(dependency)\n+\n+        # If there are any items left in the dependency graph, they are part\n+        # of a cycle.\n+        if dependencies:\n+            # The cycle detection above should have broken all cycles.\n+            # This is a fallback to ensure that we don't get into an\n+            # infinite loop.\n+            result.extend(\n+                sorted(item for item in dependencies if item not in result)\n+            )\n+\n+        return result\n+\n+    @staticmethod\n+    def merge_css(lists):\n+        \"\"\"\n+        Merge lists of CSS files, while preserving order and removing\n+        duplicates.\n+        \"\"\"\n+        if not lists:\n+            return {}\n+\n+        # The list of all unique media types.\n+        media_types = []\n+        for medium, _ in lists:\n+            if medium not in media_types:\n+                media_types.append(medium)\n+\n+        # A dictionary mapping media types to a list of all files for that\n+        # type.\n+        files_by_media = {}\n+        for media_type in media_types:\n+            files_by_media[media_type] = []\n+            for m, files in lists:\n+                if m == media_type:\n+                    files_by_media[media_type].append(files)\n+\n+        # Merge the files for each media type.\n+        result = {}\n+        for media_type in media_types:\n+            result[media_type] = Media.merge_js(files_by_media[media_type])\n+\n+        return result\n \n \n class Widget:\n\n```"}
{"instance_id": "django__django-11039", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/core/management/commands/sqlmigrate.py\n+++ b/django/core/management/commands/sqlmigrate.py\n@@ -62,7 +62,10 @@\n             sql_statements = executor.collect_sql(plan)\n         else:\n             for migration, backwards in plan:\n-                self.output_transaction = migration.atomic\n+                # A migration that is atomic on a database that doesn't support\n+                # transactional DDL will be wrapped in a transaction by the\n+                # database itself, so we don't need to do it here.\n+                self.output_transaction = migration.atomic and connection.features.can_rollback_ddl\n                 with connection.schema_editor(collect_sql=True, atomic=migration.atomic) as schema_editor:\n                     if backwards:\n                         migration.apply(project_state, schema_editor, collect_sql=True)\n--- a/tests/migrations/test_commands.py\n+++ b/tests/migrations/test_commands.py\n@@ -1,5 +1,6 @@\n import io\n from unittest import mock\n+\n from django.core.management import call_command\n from django.db import connection\n from django.db.migrations.recorder import MigrationRecorder\n@@ -219,6 +220,20 @@\n             self.assertNotIn(self.get_begin_sql(), output)\n             self.assertNotIn(self.get_commit_sql(), output)\n \n+    @mock.patch.object(connection.features, 'can_rollback_ddl', False)\n+    def test_sqlmigrate_for_database_without_transactional_ddl(self):\n+        \"\"\"\n+        sqlmigrate doesn't wrap migrations in a transaction on databases that\n+        don't support transactional DDL.\n+        \"\"\"\n+        out = io.StringIO()\n+        # Use a normal atomic migration.\n+        call_command('sqlmigrate', 'migrations', '0001', stdout=out)\n+        output = out.getvalue()\n+        self.assertIn('CREATE TABLE', output)\n+        self.assertNotIn(self.get_begin_sql(), output)\n+        self.assertNotIn(self.get_commit_sql(), output)\n+\n     @override_settings(MIGRATION_MODULES={'migrations': 'migrations.test_migrations_non_atomic'})\n     def test_sqlmigrate_non_atomic(self):\n         \"\"\"\n\n```"}
{"instance_id": "django__django-11049", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1659,8 +1659,8 @@\n     description = _(\"Duration\")\n \n     default_error_messages = {\n-        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n-                     \"[DD] [HH:[MM:]]ss[.uuuuuu] format.\"),\n+        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n+                     \"[DD] [[HH:]MM:]ss[.uuuuuu] format.\"),\n         'invalid_days': _(\"'%(value)s' value has an invalid format for days.\"),\n         'invalid_time': _(\"'%(value)s' value has an invalid format for time.\"),\n     }\n@@ -1711,3 +1711,8 @@\n                 params={'value': value},\n             )\n             return value\n+\n+    def formfield(self, **kwargs):\n+        if not self.help_text and 'help_text' not in kwargs:\n+            kwargs['help_text'] = _('Format: [DD] [[HH:]MM:]ss[.uuuuuu]')\n+        return super().formfield(**kwargs)\n--- a/django/utils/dateparse.py\n+++ b/django/utils/dateparse.py\n@@ -111,8 +111,8 @@\n def parse_duration(value):\n     \"\"\"\n     Parse a duration string and return a datetime.timedelta.\n-\n-    The format is ``[DD] [HH:[MM:]]ss[.uuuuuu]``.\n+\n+    The format is ``[DD] [[HH:]MM:]ss[.uuuuuu]``.\n     \"\"\"\n     match = duration_re.match(value)\n     if match:\n\n```"}
{"instance_id": "django__django-11099", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/contrib/auth/validators.py\n+++ b/django/contrib/auth/validators.py\n@@ -7,7 +7,7 @@\n \n @deconstructible\n class ASCIIUsernameValidator:\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only English letters, '\n         'numbers, and @/./+/-/_ characters.'\n@@ -32,7 +32,7 @@\n \n @deconstructible\n class UnicodeUsernameValidator(ASCIIUsernameValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only letters, '\n         'numbers, and @/./+/-/_ characters.'\n\n```"}
{"instance_id": "django__django-11133", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -302,6 +302,9 @@\n         # Bytestrings are returned directly.\n         if isinstance(value, bytes):\n             return value\n+        # memoryview is converted to bytes.\n+        if isinstance(value, memoryview):\n+            return bytes(value)\n         # Text is encoded using the charset.\n         if isinstance(value, str):\n             return value.encode(self.charset)\n\n```"}
{"instance_id": "django__django-11179", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -278,7 +278,12 @@\n             for model, instances in self.data.items():\n                 self.deleted_objects[model] += len(instances)\n                 pk_list = [obj.pk for obj in instances]\n                 model._base_manager.using(self.using).filter(pk__in=pk_list).delete()\n-            return sum(self.deleted_objects.values()), dict(self.deleted_objects)\n+\n+            for model, instances in self.data.items():\n+                for instance in instances:\n+                    setattr(instance, model._meta.pk.attname, None)\n+\n+            return sum(self.deleted_objects.values()), dict(self.deleted_objects)\n \n         # The logic below is very similar to Collector.collect(), but has a\n         # few differences. It's not refactored into a common method because\n\n```"}
{"instance_id": "django__django-11283", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n+++ b/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n@@ -1,13 +1,22 @@\n-from django.db import migrations\n+from django.conf import settings\n+from django.db import IntegrityError, migrations, transaction\n \n \n def update_proxy_permissions(apps, schema_editor):\n     \"\"\"\n     Update the content_type of proxy permissions to point to the concrete model.\n     \"\"\"\n     Permission = apps.get_model('auth', 'Permission')\n     ContentType = apps.get_model('contenttypes', 'ContentType')\n+    User = apps.get_model(settings.AUTH_USER_MODEL)\n+    Group = apps.get_model('auth', 'Group')\n     db_alias = schema_editor.connection.alias\n+\n+    # We need the user and group models to find references to permissions.\n+    # get_user_model() can't be used in migrations, so we have to use\n+    # the historical model.\n+    UserUserPermissions = User.user_permissions.through\n+    GroupPermission = Group.permissions.through\n     for model in apps.get_models():\n         if not model._meta.proxy:\n             continue\n@@ -31,11 +40,49 @@\n         if proxy_content_type == concrete_content_type:\n             continue\n \n-        Permission.objects.using(db_alias).filter(\n-            content_type=proxy_content_type,\n-        ).update(\n-            content_type=concrete_content_type,\n-        )\n+        # The bulk update can fail if a permission with the same codename\n+        # already exists on the concrete model. This can happen if\n+        # `create_permissions` runs before this migration.\n+        try:\n+            with transaction.atomic(using=db_alias):\n+                Permission.objects.using(db_alias).filter(\n+                    content_type=proxy_content_type,\n+                ).update(content_type=concrete_content_type)\n+        except IntegrityError:\n+            proxy_permissions = Permission.objects.using(db_alias).filter(\n+                content_type=proxy_content_type,\n+            )\n+            for p_proxy in proxy_permissions:\n+                try:\n+                    with transaction.atomic(using=db_alias):\n+                        p_proxy.content_type = concrete_content_type\n+                        p_proxy.save(using=db_alias)\n+                except IntegrityError:\n+                    p_concrete = Permission.objects.using(db_alias).get(\n+                        content_type=concrete_content_type,\n+                        codename=p_proxy.codename,\n+                    )\n+                    # Remap user permissions.\n+                    user_rels = UserUserPermissions.objects.using(db_alias).filter(permission=p_proxy)\n+                    user_ids_with_concrete_permission = set(\n+                        UserUserPermissions.objects.using(db_alias)\n+                        .filter(permission=p_concrete).values_list('user_id', flat=True)\n+                    )\n+                    user_rels.exclude(user_id__in=user_ids_with_concrete_permission).update(permission=p_concrete)\n+                    user_rels.filter(user_id__in=user_ids_with_concrete_permission).delete()\n+\n+                    # Remap group permissions.\n+                    group_rels = GroupPermission.objects.using(db_alias).filter(permission=p_proxy)\n+                    group_ids_with_concrete_permission = set(\n+                        GroupPermission.objects.using(db_alias)\n+                        .filter(permission=p_concrete).values_list('group_id', flat=True)\n+                    )\n+                    group_rels.exclude(group_id__in=group_ids_with_concrete_permission).update(permission=p_concrete)\n+                    group_rels.filter(group_id__in=group_ids_with_concrete_permission).delete()\n+\n+                    # The proxy permission is no longer needed.\n+                    p_proxy.delete(using=db_alias)\n \n \n def revert_proxy_permissions(apps, schema_editor):\n\n```"}
{"instance_id": "django__django-11422", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -193,22 +193,25 @@\n     for module in list(modules):\n         # If the module is a package, we need to find its location.\n         spec = getattr(module, '__spec__', None)\n-        if spec is None:\n-            continue\n-\n-        # We're only interested in files that can be read from the filesystem.\n-        # We can't watch things in zip archives, so we ignore them.\n-        if not getattr(spec, 'has_location', False):\n-            continue\n-\n-        # For packages, the file location is the __init__.py, if it exists.\n-        # If it doesn't, then we have a namespace package, which is a directory\n-        # of other packages and modules. In this case, we're not interested in\n-        # the package itself, but its subpackages and modules, which will be\n-        # found as they are also in sys.modules.\n-        if spec.submodule_search_locations is not None:\n-            if spec.origin is not None and spec.origin.endswith('__init__.py'):\n-                location = spec.origin\n+        # __spec__ can be None for __main__ module.\n+        if spec is None:\n+            if module.__name__ == '__main__' and hasattr(module, '__file__'):\n+                location = module.__file__\n             else:\n                 continue\n         else:\n-            location = spec.origin\n+            # We're only interested in files that can be read from the filesystem.\n+            # We can't watch things in zip archives, so we ignore them.\n+            if not getattr(spec, 'has_location', False):\n+                continue\n+\n+            # For packages, the file location is the __init__.py, if it exists.\n+            # If it doesn't, then we have a namespace package, which is a\n+            # directory of other packages and modules. In this case, we're not\n+            # interested in the package itself, but its subpackages and\n+            # modules, which will be found as they are also in sys.modules.\n+            if spec.submodule_search_locations is not None:\n+                if spec.origin is not None and spec.origin.endswith('__init__.py'):\n+                    location = spec.origin\n+                else:\n+                    continue\n+            else:\n+                location = spec.origin\n \n         if location is None:\n             continue\n\n```"}
{"instance_id": "django__django-11564", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -1,12 +1,14 @@\n import os\n import os.path\n+from urllib.parse import urljoin, urlparse\n \n from django.conf import settings\n from django.core.exceptions import ImproperlyConfigured\n from django.core.signals import setting_changed\n+from django.urls import get_script_prefix\n from django.utils._os import abspathu, safe_join\n from django.utils.deconstruct import deconstructible\n from django.utils.encoding import filepath_to_uri\n-from django.utils.six.moves.urllib.parse import urljoin\n \n __all__ = ['Storage', 'FileSystemStorage', 'get_storage_class', 'default_storage']\n \n@@ -107,4 +109,9 @@\n     def url(self, name):\n         if self.base_url is None:\n             raise ValueError(\"This file is not accessible via a URL.\")\n-        return urljoin(self.base_url, filepath_to_uri(name))\n+        base_url = self.base_url\n+        if not urlparse(base_url).netloc and not urlparse(base_url).scheme:\n+            base_url = get_script_prefix() + base_url.lstrip('/')\n+        return urljoin(base_url, filepath_to_uri(name))\n+\n+\n\n```"}
{"instance_id": "django__django-11583", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -129,17 +129,29 @@\n         if spec and spec.has_location and spec.origin:\n             path = Path(spec.origin)\n             if path.is_file():\n                 # If the path is a file, we're done.\n-                results.add(path.resolve().absolute())\n+                try:\n+                    results.add(path.resolve().absolute())\n+                except ValueError:\n+                    # path.resolve() may raise ValueError on some systems,\n+                    # e.g. when the path contains a null byte.\n+                    pass\n             else:\n                 # If it's a directory, we need to walk it.\n                 for p in path.glob('**/*.py'):\n-                    results.add(p.resolve().absolute())\n+                    try:\n+                        results.add(p.resolve().absolute())\n+                    except ValueError:\n+                        # p.resolve() may raise ValueError on some systems,\n+                        # e.g. when the path contains a null byte.\n+                        pass\n \n     for path in extra_files:\n-        results.add(Path(path).resolve().absolute())\n+        try:\n+            results.add(Path(path).resolve().absolute())\n+        except ValueError:\n+            # Path(path).resolve() may raise ValueError on some systems,\n+            # e.g. when the path contains a null byte.\n+            pass\n \n     return iter(results)\n \n\n```"}
{"instance_id": "django__django-11620", "model_name_or_path": "gemini-2.5-pro", "model_patch": "```diff\n--- a/django/views/debug.py\n+++ b/django/views/debug.py\n@@ -482,13 +482,12 @@\n         resolver = get_resolver()\n         tried = resolver.resolve_error_handler(request, exception)\n-    except Resolver404 as e:\n-        tried = e.args[0]['tried']\n-    except Exception:\n-        # This is unexpected.\n-        tried = []\n-        # Let the exception bubble up to be handled by the 500 handler.\n-        raise\n+    except Exception as e:\n+        if isinstance(e, Resolver404):\n+            tried = e.args[0]['tried']\n+        else:\n+            # A 404 may be raised trying to resolve the URL. If that happens,\n+            # we want to show the 404 template with a simple message.\n+            tried = []\n \n     # The resolver should never fail to resolve the technical 404 view.\n     callback, param_dict = get_resolver(None).resolve('technical-404/')\n\n```"}
